{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-28T18:27:19.490661Z","iopub.status.busy":"2022-04-28T18:27:19.490402Z","iopub.status.idle":"2022-04-28T18:27:33.0568Z","shell.execute_reply":"2022-04-28T18:27:33.056048Z","shell.execute_reply.started":"2022-04-28T18:27:19.490584Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","from textblob import TextBlob\n","\n","import seaborn as sns\n","import matplotlib.style as style \n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn import set_config\n","set_config(display=\"diagram\")\n","import time\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:27:33.083886Z","iopub.status.busy":"2022-04-28T18:27:33.083526Z","iopub.status.idle":"2022-04-28T18:27:40.150683Z","shell.execute_reply":"2022-04-28T18:27:40.14991Z","shell.execute_reply.started":"2022-04-28T18:27:33.083838Z"},"trusted":true},"outputs":[],"source":["#loading data\n","df = pd.read_csv(\"../data/train-balanced-sarcasm.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:27:40.158859Z","iopub.status.busy":"2022-04-28T18:27:40.158453Z","iopub.status.idle":"2022-04-28T18:27:40.190871Z","shell.execute_reply":"2022-04-28T18:27:40.190188Z","shell.execute_reply.started":"2022-04-28T18:27:40.158823Z"},"trusted":true},"outputs":[],"source":["#loading embeddings\n","emb = np.load('../data/user_gcca_embeddings.npz', )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:27:40.192388Z","iopub.status.busy":"2022-04-28T18:27:40.192129Z","iopub.status.idle":"2022-04-28T18:27:40.392643Z","shell.execute_reply":"2022-04-28T18:27:40.391902Z","shell.execute_reply.started":"2022-04-28T18:27:40.192353Z"},"trusted":true},"outputs":[],"source":["# emb['ids'][4].decode('UTF-8')[1:-1] in df['author']\n","l = []\n","for val in emb['ids']:\n","    l.append(val.decode('UTF-8')[1:-1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:27:40.394182Z","iopub.status.busy":"2022-04-28T18:27:40.393927Z","iopub.status.idle":"2022-04-28T18:27:40.808315Z","shell.execute_reply":"2022-04-28T18:27:40.807521Z","shell.execute_reply.started":"2022-04-28T18:27:40.394148Z"},"trusted":true},"outputs":[],"source":["userincommon = list(set(l) & set(set(df['author'])))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:27:40.811254Z","iopub.status.busy":"2022-04-28T18:27:40.810973Z","iopub.status.idle":"2022-04-28T18:27:42.861268Z","shell.execute_reply":"2022-04-28T18:27:42.860439Z","shell.execute_reply.started":"2022-04-28T18:27:40.811215Z"},"trusted":true},"outputs":[],"source":["df.index = df['author']\n","uniquedf = df.loc[userincommon]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:27:42.863387Z","iopub.status.busy":"2022-04-28T18:27:42.862821Z","iopub.status.idle":"2022-04-28T18:27:42.926623Z","shell.execute_reply":"2022-04-28T18:27:42.925837Z","shell.execute_reply.started":"2022-04-28T18:27:42.863342Z"},"trusted":true},"outputs":[],"source":["embdic = {v: k for k, v in enumerate(l)}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:27:42.935074Z","iopub.status.busy":"2022-04-28T18:27:42.934721Z","iopub.status.idle":"2022-04-28T18:28:20.123616Z","shell.execute_reply":"2022-04-28T18:28:20.122853Z","shell.execute_reply.started":"2022-04-28T18:27:42.935037Z"},"trusted":true},"outputs":[],"source":["userindexes = []\n","\n","for val in uniquedf.iterrows():\n","\n","    userindexes.append(embdic[val[0]])\n","uniquedf['userindexes'] = userindexes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train,X_test,Y_train,Y_test = train_test_split(uniquedf['comment', 'parent_comment', 'userindexes'],random_state=42)\n","datasettouse = None\n","datasettype = 'parent' # can be either parent, or parentresponse\n","if datasettype == 'parent':\n","    X_train['text'] = df['parent_comment'].astype(str)\n","elif datasettype == 'parentresponse':\n","    X_train['text'] =df['parent_comment'].astype(str)+\" \"+df['comment'].astype(str)\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:29:06.750552Z","iopub.status.busy":"2022-04-28T18:29:06.750279Z","iopub.status.idle":"2022-04-28T18:29:08.257425Z","shell.execute_reply":"2022-04-28T18:29:08.256738Z","shell.execute_reply.started":"2022-04-28T18:29:06.75052Z"},"trusted":true},"outputs":[],"source":["# !pip install transformers\n","from transformers import AutoTokenizer, AutoModel\n","\n","import random\n","import numpy as np\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForSequenceClassification, AdamW, BertConfig, AutoModelForSequenceClassification\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:29:09.047521Z","iopub.status.busy":"2022-04-28T18:29:09.047262Z","iopub.status.idle":"2022-04-28T18:29:17.167811Z","shell.execute_reply":"2022-04-28T18:29:17.167065Z","shell.execute_reply.started":"2022-04-28T18:29:09.047492Z"},"trusted":true},"outputs":[],"source":["deviceno = 0\n","modelname = 'bert-base-uncased'\n","\n","max_length = 128\n","batch_size = 8\n","epochs = 1\n","import pandas as pd\n","# df = pd.read_csv('balancedSpaceSep.csv')\n","\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(modelname)\n","\n","import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset\n","# Load the dataset into a pandas dataframe.\n","\n","\n","\n","# Create sentence and label lists\n","train_sentences = X_train['text'].astype(str)\n","test_sentences = X_test['text'].astype(str)\n","train_labels = Y_train\n","test_labels = Y_test\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:29:32.559465Z","iopub.status.busy":"2022-04-28T18:29:32.559204Z","iopub.status.idle":"2022-04-28T18:33:38.809269Z","shell.execute_reply":"2022-04-28T18:33:38.808352Z","shell.execute_reply.started":"2022-04-28T18:29:32.559435Z"},"trusted":true},"outputs":[],"source":["\n","# For every sentence...\n","input_ids = []\n","attention_masks = []\n","\n","for sent in train_sentences:\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_length,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(list(train_labels))\n","trainuserindexes = torch.tensor(X_train['userindexes'])\n","labels = labels.to(torch.int64)\n","trainuserindexes = trainuserindexes.to(torch.int64)\n","# labels = F.one_hot(labels.to(torch.int64))\n","\n","train_dataset = TensorDataset(input_ids, attention_masks, labels,trainuserindexes)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:33:38.814796Z","iopub.status.busy":"2022-04-28T18:33:38.814392Z","iopub.status.idle":"2022-04-28T18:34:49.512863Z","shell.execute_reply":"2022-04-28T18:34:49.512172Z","shell.execute_reply.started":"2022-04-28T18:33:38.814749Z"},"trusted":true},"outputs":[],"source":["\n","input_ids = []\n","attention_masks = []\n","\n","reduceto=len(test_labels)\n","test_labels = test_labels[:reduceto]\n","test_sentences = test_sentences[:reduceto]\n","testuserindexes = torch.tensor(X_train['userindexes'])[:reduceto]\n","for sent in test_sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 64,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(list(test_labels))\n","labels = labels.to(torch.int64)\n","testuserindexes = testuserindexes.to(torch.int64)\n","\n","# labels = F.one_hot(labels.to(torch.int64))\n","val_dataset = TensorDataset(input_ids, attention_masks, labels, testuserindexes)\n","\n","\n","# import torch.nn.functional as F\n","# # lab = torch.FloatTensor(labels.shape[0], 2)\n","# # lab.scatter_(1, labels.int() ,1)\n","\n","# from torch.utils.data import TensorDataset, random_split\n","\n","# # Combine the training inputs into a TensorDataset.\n","# dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# # Create a 90-10 train-validation split.\n","\n","# # Calculate the number of samples to include in each set.\n","# train_size = int(0.9 * len(dataset))\n","# val_size = len(dataset) - train_size\n","\n","# # Divide the dataset by randomly selecting samples.\n","# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# print('{:>5,} training samples'.format(train_size))\n","# print('{:>5,} validation samples'.format(val_size))\n","val_dataset\n","\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:34:49.524067Z","iopub.status.busy":"2022-04-28T18:34:49.523543Z","iopub.status.idle":"2022-04-28T18:34:49.534511Z","shell.execute_reply":"2022-04-28T18:34:49.533706Z","shell.execute_reply.started":"2022-04-28T18:34:49.524028Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","class BERTCascade(nn.Module):\n","    def __init__(self,\n","                 bert_encoder: nn.Module,\n","                 user_emb,\n","                 enc_hid_dim=768, #default embedding size\n","                 outputs=2,\n","                 dropout=0.1):\n","        super().__init__()\n","\n","        self.bert_encoder = bert_encoder\n","\n","        self.enc_hid_dim = enc_hid_dim\n","        self.useremb = nn.Embedding(user_emb.shape[0], user_emb.shape[1])\n","        self.useremb.weight = user_emb\n","        \n","        \n","        \n","        ### YOUR CODE HERE ### \n","        self.fc1 = nn.Linear(self.enc_hid_dim, self.enc_hid_dim)\n","        self.fc2 = nn.Linear(self.enc_hid_dim+user_emb.shape[1], outputs)\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","\n","\n","    def forward(self,\n","                src,\n","                mask, user_indexes):\n","        bert_output = self.bert_encoder(src, mask)\n","\n","        ### YOUR CODE HERE ###\n","        hidden_state = bert_output.last_hidden_state\n","        pooled_output = hidden_state[:,0]\n","        pooled_output = self.fc1(pooled_output)  \n","        user_emb = self.useremb(user_indexes)\n","        pooled_output = torch.cat((pooled_output, user_emb), dim=1)\n","        pooled_output = nn.ReLU()(pooled_output)  \n","        pooled_output = self.dropout(pooled_output) \n","        logits = self.fc2(pooled_output)\n","        return logits\n","\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:34:49.53624Z","iopub.status.busy":"2022-04-28T18:34:49.535977Z","iopub.status.idle":"2022-04-28T18:35:11.910379Z","shell.execute_reply":"2022-04-28T18:35:11.909634Z","shell.execute_reply.started":"2022-04-28T18:34:49.536205Z"},"trusted":true},"outputs":[],"source":["model2 = AutoModel.from_pretrained(modelname)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:49:56.656679Z","iopub.status.busy":"2022-04-28T18:49:56.656413Z","iopub.status.idle":"2022-04-28T18:49:58.625875Z","shell.execute_reply":"2022-04-28T18:49:58.625068Z","shell.execute_reply.started":"2022-04-28T18:49:56.656647Z"},"trusted":true},"outputs":[],"source":["# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","# train_dataset = train_dataset[:int(len(list(train_dataset))/50)]\n","# model2 = AutoModel.from_pretrained(modelname)\n","batch_size = 24\n","train_dataloader = DataLoader(\n","#             train_dataset,  # The training samples.\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","\n","# val_dataset =    val_dataset[:int(len(list(val_dataset))/50)]\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","device = 'cpu'\n","if torch.cuda.is_available():\n","    device = 'cuda'+':'+str(deviceno)\n","model2 = model2.to(device)\n","print(device)\n","model = BERTCascade(model2, nn.Parameter(torch.tensor(emb['G'].astype(np.float32)).to(device)))\n","model = model.to(device)\n","# Tell pytorch to run this model on the GPU.\n","# model.cuda()\n","\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","import numpy as np\n","\n","\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T18:50:10.178223Z","iopub.status.busy":"2022-04-28T18:50:10.17797Z","iopub.status.idle":"2022-04-28T21:26:09.805267Z","shell.execute_reply":"2022-04-28T21:26:09.801561Z","shell.execute_reply.started":"2022-04-28T18:50:10.178194Z"},"trusted":true},"outputs":[],"source":["\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","epochs = 1\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","import torch.nn.functional as F\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 1000 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        b_userindexes = batch[3].to(device)\n","   \n","        optimizer.zero_grad()\n","\n","\n","        vals = model(b_input_ids, \n","                            #  token_type_ids=None, \n","                             b_input_mask, \n","                            b_userindexes)\n","        loss = F.cross_entropy(vals, b_labels)\n","#         loss = vals.loss\n","#         logits = vals.logits\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        # print(loss, logits, vals)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        \n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        b_userindexes = batch[3].to(device)\n","\n","\n","        with torch.no_grad():        \n","\n","\n","            # (loss, logits) = model(b_input_ids, \n","            #                       #  token_type_ids=None, \n","            #                        attention_mask=b_input_mask,\n","            #                        labels=b_labels)\n","            vals = model(b_input_ids, \n","                            #  token_type_ids=None, \n","                             b_input_mask, \n","        #                              labels=b_labels,\n","                            b_userindexes)\n","        #         loss = vals.loss\n","        #         logits = vals.logits\n","\n","\n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = vals.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","\n","        # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["total_eval_accuracy = 0\n","total_eval_loss = 0\n","nb_eval_steps = 0\n","\n","# Evaluate data for one epoch\n","model.eval()\n","for i, batch in enumerate(validation_dataloader):\n","\n","\n","\n","#\n","# `batch` contains three pytorch tensors:\n","#   [0]: input ids \n","#   [1]: attention masks\n","#   [2]: labels \n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    b_userindexes = batch[3].to(device)\n","\n","\n","\n","    with torch.no_grad():        \n","\n","\n","        # (loss, logits) = model(b_input_ids, \n","        #                       #  token_type_ids=None, \n","        #                        attention_mask=b_input_mask,\n","        #                        labels=b_labels)\n","        vals = model(b_input_ids, \n","\n","                         b_input_mask, \n","\n","                        b_userindexes)\n","\n","\n","\n","    # Accumulate the validation loss.\n","    total_eval_loss += loss.item()\n","\n","    # Move logits and labels to CPU\n","    logits = vals.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Calculate the accuracy for this batch of test sentences, and\n","    # accumulate it over all batches.\n","    pred_flat = np.argmax(logits, axis=1).flatten()\n","    labels_flat = label_ids.flatten()\n","    l.append(pred_flat == labels_flat)\n","    print(total_eval_accuracy/(i+1))\n","\n","    total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","\n","    # Report the final accuracy for this validation run.\n","avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","\n","print(\"  VALIDATION ACCURACY: {0:.2f}\".format(avg_val_accuracy))\n","\n","# Calculate the average loss over all of the batches.\n","avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","# Measure how long the validation run took.\n","validation_time = format_time(time.time() - t0)\n","\n","print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","print(\"  Validation took: {:}\".format(validation_time))\n","\n","# Record all statistics from this epoch.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:52:02.39379Z","iopub.status.busy":"2022-04-28T21:52:02.393507Z","iopub.status.idle":"2022-04-28T21:54:23.870731Z","shell.execute_reply":"2022-04-28T21:54:23.869845Z","shell.execute_reply.started":"2022-04-28T21:52:02.393748Z"},"trusted":true},"outputs":[],"source":["\n","#Train Logistic Regression Model\n","lr_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2))),('clf',  LogisticRegression(random_state= 42, solver='liblinear'))])\n","lr_clf.fit(train_sentences,train_labels)\n","# print(f\"The accuracy on the training set is: {lr_clf.score(X_train,Y_train)}\")\n","print(f\"The accuracy on the test set is:  {lr_clf.score(test_sentences,test_labels)}\")\n","Y_preds = lr_clf.predict(test_sentences)\n","equals = test_labels==Y_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T22:04:45.077637Z","iopub.status.busy":"2022-04-28T22:04:45.076843Z","iopub.status.idle":"2022-04-28T22:04:45.089797Z","shell.execute_reply":"2022-04-28T22:04:45.089024Z","shell.execute_reply.started":"2022-04-28T22:04:45.077584Z"},"trusted":true},"outputs":[],"source":["first = np.sum((np.vstack((equals, val2)).T==np.array([True, True])).all(axis=1))\n","second = np.sum((np.vstack((equals, val2)).T==np.array([True, False])).all(axis=1))\n","third = np.sum((np.vstack((equals, val2)).T==np.array([False, True])).all(axis=1))\n","fourth = np.sum((np.vstack((equals, val2)).T==np.array([False, False])).all(axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T22:05:34.102306Z","iopub.status.busy":"2022-04-28T22:05:34.101749Z","iopub.status.idle":"2022-04-28T22:05:43.066128Z","shell.execute_reply":"2022-04-28T22:05:43.065155Z","shell.execute_reply.started":"2022-04-28T22:05:34.102266Z"},"trusted":true},"outputs":[],"source":["# !pip install statsmodels\n","from statsmodels.stats.contingency_tables import mcnemar\n","# define contingency table\n","table = [[first, second],[third, fourth]]\n","# calculate mcnemar test\n","result = mcnemar(table, exact=True)\n","# summarize the finding\n","print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n","# interpret the p-value\n","alpha = 0.05\n","if result.pvalue > alpha:\n","    print('Same proportions of errors (fail to reject H0)')\n","else:\n","    print('Different proportions of errors (reject H0)')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
