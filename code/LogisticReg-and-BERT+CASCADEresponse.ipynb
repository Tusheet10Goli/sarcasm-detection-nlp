{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-29T00:53:37.874155Z","iopub.status.busy":"2022-04-29T00:53:37.873732Z","iopub.status.idle":"2022-04-29T00:53:46.874531Z","shell.execute_reply":"2022-04-29T00:53:46.873677Z","shell.execute_reply.started":"2022-04-29T00:53:37.874061Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","from textblob import TextBlob\n","\n","import seaborn as sns\n","import matplotlib.style as style \n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn import set_config\n","set_config(display=\"diagram\")\n","import time\n","\n","import json\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:53:46.901084Z","iopub.status.busy":"2022-04-29T00:53:46.900873Z","iopub.status.idle":"2022-04-29T00:55:14.382262Z","shell.execute_reply":"2022-04-29T00:55:14.381383Z","shell.execute_reply.started":"2022-04-29T00:53:46.901059Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pickle\n","from collections import defaultdict\n","import sys, re\n","import pandas as pd\n","import json\n","\n","# noinspection PyCompatibility\n","from builtins import range\n","\n","COMMENTS_FILE = \"../data/comments.json\"\n","TRAIN_MAP_FILE = \"../data/my_train_balanced.csv\"\n","TEST_MAP_FILE = \"../data/my_test_balanced.csv\"\n","\n","def build_data_cv(data_folder, cv=10, clean_string=True):\n","    \"\"\"\n","    Loads data\n","    \"\"\"\n","    revs = []\n","\n","    sarc_train_file = data_folder[0]\n","    sarc_test_file = data_folder[1]\n","    \n","    train_data = np.asarray(pd.read_csv(sarc_train_file, header=None))\n","    test_data = np.asarray(pd.read_csv(sarc_test_file, header=None))\n","    print('starting to load json')\n","#     comments = json.loads(open(COMMENTS_FILE).read())\n","    f = open('../data/comments0.json')\n","    comments0 = json.load(f)\n","    f = open('../data/comments1.json')\n","    comments1 = json.load(f)\n","    f = open('../data/comments2.json')\n","    comments2 = json.load(f)\n","    f = open('./data/comments3.json')\n","    comments3 = json.load(f)\n","    comments  = {}\n","    comments.update(comments0)\n","    comments0 = \"\"\n","    comments.update(comments1)\n","    comments1 = \"\"\n","    comments.update(comments2)\n","    comments2 = \"\"\n","    comments.update(comments3)\n","    comments3 = \"\"\n","#     vocab = defaultdict(float)\n","\n","\n","    print('done loading comment json')\n","    for line in train_data: \n","        rev = []\n","        label_str = line[2]\n","        if( label_str == 0):\n","            label = 0\n","        else:\n","            label = 1\n","        rev.append(comments[line[0]]['text'].strip())\n","        if clean_string:\n","            orig_rev = clean_str(\" \".join(rev))\n","        else:\n","            orig_rev = \" \".join(rev).lower()\n","#         words = set(orig_rev.split())\n","#         for word in words:\n","#             vocab[word] += 1\n","        orig_rev = (orig_rev.split())[0:100]\n","        orig_rev = \" \".join(orig_rev)\n","        datum  = {\"y\":int(1), \n","                  \"id\":line[0],\n","                  \"text\": orig_rev,\n","                  \"author\": comments[line[0]]['author'],\n","                  \"topic\": comments[line[0]]['subreddit'],\n","                  \"label\": label,\n","                  \"num_words\": len(orig_rev.split()),\n","                  \"split\": int(1)}\n","        revs.append(datum)\n","    print('done train')\n","\n","    for line in test_data:\n","        rev = []\n","        label_str = line[2]\n","        if( label_str == 0):\n","            label = 0\n","        else:\n","            label = 1\n","        rev.append(comments[line[0]]['text'].strip())\n","        if clean_string:\n","            orig_rev = clean_str(\" \".join(rev))\n","        else:\n","            orig_rev = \" \".join(rev).lower()\n","#         words = set(orig_rev.split())\n","#         for word in words:\n","#             vocab[word] += 1\n","        orig_rev = (orig_rev.split())[0:100]\n","        orig_rev = \" \".join(orig_rev)\n","        datum  = {\"y\":int(1),\n","                  \"id\": line[0], \n","                  \"text\": orig_rev,  \n","                  \"author\": comments[line[0]]['author'],\n","                  \"topic\": comments[line[0]]['subreddit'],\n","                  \"label\": label,\n","                  \"num_words\": len(orig_rev.split()),                      \n","                  \"split\": int(0)}\n","        revs.append(datum)\n","        \n","    comments = \"\"\n","    return revs\n","\n","\n","\n","def clean_str(string, TREC=False):\n","    \"\"\"\n","    Tokenization/string cleaning for all datasets except for SST.\n","    Every dataset is lower cased except for TREC\n","    \"\"\"\n","    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n","    string = re.sub(r\"\\'s\", \" \\'s\", string) \n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n","    string = re.sub(r\"\\'re\", \" \\'re\", string) \n","    string = re.sub(r\"\\'d\", \" \\'d\", string) \n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n","    string = re.sub(r\",\", \" , \", string) \n","    string = re.sub(r\"!\", \" ! \", string) \n","    string = re.sub(r\"\\(\", \" \\( \", string) \n","    string = re.sub(r\"\\)\", \" \\) \", string) \n","    string = re.sub(r\"\\?\", \" \\? \", string) \n","    string = re.sub(r\"\\s{2,}\", \" \", string)    \n","    return string.strip() if TREC else string.strip().lower()\n","\n","def clean_str_sst(string):\n","    \"\"\"\n","    Tokenization/string cleaning for the SST dataset\n","    \"\"\"\n","    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)   \n","    string = re.sub(r\"\\s{2,}\", \" \", string)    \n","    return string.strip().lower()\n","\n","\n","\n","# w2v_file = sys.argv[1]    \n","data_folder = [TRAIN_MAP_FILE,TEST_MAP_FILE] \n","print(\"loading data...\")\n","revs = build_data_cv(data_folder,  cv=10, clean_string=True)\n","max_l = np.max(pd.DataFrame(revs)[\"num_words\"])\n","print(\"data loaded!\")\n","print(\"number of sentences: \" + str(len(revs)))\n","# print(\"vocab size: \" + str(len(vocab)))\n","print(\"max sentence length: \" + str(max_l))\n","print(\"loading word2vec vectors...\")\n","\n","print(\"dataset created!\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:55:14.385149Z","iopub.status.busy":"2022-04-29T00:55:14.384497Z","iopub.status.idle":"2022-04-29T00:55:14.408632Z","shell.execute_reply":"2022-04-29T00:55:14.407923Z","shell.execute_reply.started":"2022-04-29T00:55:14.385107Z"},"trusted":true},"outputs":[],"source":["wgcca_embeddings = np.load('../data/user_gcca_embeddings.npz')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:55:14.410374Z","iopub.status.busy":"2022-04-29T00:55:14.409972Z","iopub.status.idle":"2022-04-29T00:55:22.617186Z","shell.execute_reply":"2022-04-29T00:55:22.616377Z","shell.execute_reply.started":"2022-04-29T00:55:14.410335Z"},"trusted":true},"outputs":[],"source":["import csv\n","ids = np.concatenate((np.array([\"unknown\"]), wgcca_embeddings['ids']), axis=0)\n","user_embeddings = wgcca_embeddings['G']\n","unknown_vector = np.random.normal(size=(1,100))\n","user_embeddings = np.concatenate((unknown_vector, user_embeddings), axis=0)\n","user_embeddings = user_embeddings.astype(dtype='float32')\n","\n","wgcca_dict = {}\n","for i in range(len(ids)):\n","    wgcca_dict[ids[i]] = int(i)\n","\n","csv_reader = csv.reader(open(\"../data/discourse.csv\"))\n","topic_embeddings = []\n","topic_ids = []\n","for line in csv_reader:\n","    topic_ids.append(line[0])\n","    topic_embeddings.append(line[1:])\n","topic_embeddings = np.asarray(topic_embeddings)\n","topic_embeddings_size = len(topic_embeddings[0])\n","topic_embeddings = topic_embeddings.astype(dtype='float32')\n","print(\"topic emb size: \",topic_embeddings_size)\n","\n","topics_dict = {}\n","for i in range(len(topic_ids)):\n","    try:\n","        topics_dict[topic_ids[i]] = int(i)\n","    except TypeError:\n","        print(i)\n","\n","max_l = 100\n","\n","x_text = []\n","author_text_id = []\n","topic_text_id = []\n","y = []\n","\n","test_x = []\n","test_topic = []\n","test_author = []\n","test_y = []\n","\n","for i in range(len(revs)):\n","    if revs[i]['split']==1:\n","        x_text.append(revs[i]['text'])\n","        try:\n","            author_text_id.append(wgcca_dict['\"'+revs[i]['author']+'\"'])\n","        except KeyError:\n","            try:\n","                author_text_id.append(wgcca_dict[revs[i]['author']])\n","            except KeyError:\n","                author_text_id.append(0)\n","        try:\n","            topic_text_id.append(topics_dict[revs[i]['topic']])\n","        except KeyError:\n","            topic_text_id.append(0)\n","        temp_y = revs[i]['label']\n","        y.append(temp_y)\n","    else:\n","        test_x.append(revs[i]['text'])\n","        try:\n","            test_author.append(wgcca_dict['\"'+revs[i]['author']+'\"'])\n","        except:\n","            test_author.append(0)\n","        try:\n","            test_topic.append(topics_dict[revs[i]['topic']])\n","        except:\n","            test_topic.append(0)\n","        test_y.append(revs[i]['label'])  \n","\n","y = np.asarray(y)\n","test_y = np.asarray(test_y)\n","\n","y_test = test_y\n","\n","topic_train = np.asarray(topic_text_id)\n","topic_test = np.asarray(test_topic)\n","author_train = np.asarray(author_text_id)\n","author_test = np.asarray(test_author)\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:55:22.618838Z","iopub.status.busy":"2022-04-29T00:55:22.618355Z","iopub.status.idle":"2022-04-29T00:55:22.625469Z","shell.execute_reply":"2022-04-29T00:55:22.624773Z","shell.execute_reply.started":"2022-04-29T00:55:22.618800Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:55:22.627201Z","iopub.status.busy":"2022-04-29T00:55:22.626612Z","iopub.status.idle":"2022-04-29T00:55:23.294928Z","shell.execute_reply":"2022-04-29T00:55:23.294246Z","shell.execute_reply.started":"2022-04-29T00:55:22.627163Z"},"trusted":true},"outputs":[],"source":["# !pip install transformers\n","from transformers import AutoTokenizer, AutoModel\n","\n","import random\n","import numpy as np\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForSequenceClassification, AdamW, BertConfig, AutoModelForSequenceClassification\n","\n","\n","topic_train.shape, topic_test.shape, author_train.shape, author_test"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:55:23.296720Z","iopub.status.busy":"2022-04-29T00:55:23.296468Z","iopub.status.idle":"2022-04-29T00:55:26.478135Z","shell.execute_reply":"2022-04-29T00:55:26.477358Z","shell.execute_reply.started":"2022-04-29T00:55:23.296686Z"},"trusted":true},"outputs":[],"source":["deviceno = 0\n","modelname = 'bert-base-uncased'\n","#modelname = \"cardiffnlp/twitter-roberta-base-offensive\"\n","#modelname = 'microsoft/deberta-base'\n","# modelname = 'facebook/bart-large'\n","#modelname = 'unitary/toxic-bert'\n","max_length = 128\n","batch_size = 8\n","epochs = 4\n","import pandas as pd\n","# df = pd.read_csv('balancedSpaceSep.csv')\n","\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(modelname)\n","\n","import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# Load the dataset into a pandas dataframe.\n","\n","# Report the number of sentences.\n","# print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Create sentence and label lists\n","\n","\n","\n","train_sentences = x_text\n","test_sentences = test_x\n","train_labels = y\n","test_labels = test_y\n","\n","\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:55:26.479528Z","iopub.status.busy":"2022-04-29T00:55:26.479268Z","iopub.status.idle":"2022-04-29T00:55:26.484809Z","shell.execute_reply":"2022-04-29T00:55:26.483949Z","shell.execute_reply.started":"2022-04-29T00:55:26.479495Z"},"trusted":true},"outputs":[],"source":["len(train_labels)# train_labels = train_labels[:int(len(train_labels)/50)]\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:55:26.486571Z","iopub.status.busy":"2022-04-29T00:55:26.486161Z","iopub.status.idle":"2022-04-29T00:56:09.592112Z","shell.execute_reply":"2022-04-29T00:56:09.591296Z","shell.execute_reply.started":"2022-04-29T00:55:26.486534Z"},"trusted":true},"outputs":[],"source":["\n","# For every sentence...\n","input_ids = []\n","attention_masks = []\n","# train_labels = train_labels[:int(len(train_labels)/50)]\n","# train_sentences = train_sentences[:int(len(train_sentences)/50)]\n","for sent in train_sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_length,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(list(train_labels))\n","labels = labels.to(torch.int64)\n","trainuserindexes = torch.tensor(author_train)\n","trainuserindexes = trainuserindexes.to(torch.int64)\n","traintopicindexes = torch.tensor(topic_train)\n","traintopicindexes = traintopicindexes.to(torch.int64)\n","# labels = F.one_hot(labels.to(torch.int64))\n","\n","train_dataset = TensorDataset(input_ids, attention_masks, labels,trainuserindexes, traintopicindexes)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:56:09.594216Z","iopub.status.busy":"2022-04-29T00:56:09.593708Z","iopub.status.idle":"2022-04-29T00:56:23.708743Z","shell.execute_reply":"2022-04-29T00:56:23.707941Z","shell.execute_reply.started":"2022-04-29T00:56:09.594176Z"},"trusted":true},"outputs":[],"source":["\n","input_ids = []\n","attention_masks = []\n","reduceto = len(test_labels)\n","test_labels = test_labels[:reduceto]\n","test_sentences = test_sentences[:reduceto]\n","# testuserindexes = torch.tensor(X_train['userindexes'])[:reduceto]\n","testuserindexes = author_test[:reduceto]\n","testtopicindexes = topic_test[:reduceto]\n","for sent in test_sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 64,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(list(test_labels))\n","labels = labels.to(torch.int64)\n","\n","testuserindexes = torch.tensor(testuserindexes)\n","testuserindexes = testuserindexes.to(torch.int64)\n","testtopicindexes = torch.tensor(testtopicindexes)\n","testtopicindexes = testtopicindexes.to(torch.int64)\n","\n","\n","# labels = F.one_hot(labels.to(torch.int64))\n","val_dataset = TensorDataset(input_ids, attention_masks, labels, testuserindexes, testtopicindexes)\n","\n","\n","# import torch.nn.functional as F\n","# # lab = torch.FloatTensor(labels.shape[0], 2)\n","# # lab.scatter_(1, labels.int() ,1)\n","\n","# from torch.utils.data import TensorDataset, random_split\n","\n","# # Combine the training inputs into a TensorDataset.\n","# dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# # Create a 90-10 train-validation split.\n","\n","# # Calculate the number of samples to include in each set.\n","# train_size = int(0.9 * len(dataset))\n","# val_size = len(dataset) - train_size\n","\n","# # Divide the dataset by randomly selecting samples.\n","# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# print('{:>5,} training samples'.format(train_size))\n","# print('{:>5,} validation samples'.format(val_size))\n","val_dataset\n","\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:56:23.713110Z","iopub.status.busy":"2022-04-29T00:56:23.712542Z","iopub.status.idle":"2022-04-29T00:56:23.723802Z","shell.execute_reply":"2022-04-29T00:56:23.723025Z","shell.execute_reply.started":"2022-04-29T00:56:23.713050Z"},"trusted":true},"outputs":[],"source":["\n","class BERTCascade(nn.Module):\n","    def __init__(self,\n","                 bert_encoder: nn.Module,\n","                 user_emb,\n","                 topic_emb,\n","                 enc_hid_dim=768, #default embedding size\n","                 outputs=2,\n","                 dropout=0.1):\n","        super().__init__()\n","\n","        self.bert_encoder = bert_encoder\n","\n","        self.enc_hid_dim = enc_hid_dim\n","        self.useremb = nn.Embedding(user_emb.shape[0], user_emb.shape[1])\n","        self.useremb.weight = user_emb\n","        self.topicemb = nn.Embedding(topic_emb.shape[0], topic_emb.shape[1])\n","        self.topicemb.weight = user_emb\n","        \n","        \n","        ### YOUR CODE HERE ### \n","        self.fc1 = nn.Linear(self.enc_hid_dim, self.enc_hid_dim)\n","        self.fc2 = nn.Linear(self.enc_hid_dim+user_emb.shape[1]+topic_emb.shape[1], outputs)\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","\n","\n","    def forward(self,\n","                src,\n","                mask, user_indexes, topic_indexes):\n","        bert_output = self.bert_encoder(src, mask)\n","\n","        ### YOUR CODE HERE ###\n","        hidden_state = bert_output.last_hidden_state\n","        pooled_output = hidden_state[:,0]\n","        pooled_output = self.fc1(pooled_output)  \n","        user_emb = self.useremb(user_indexes)\n","        topic_emb = self.topicemb(topic_indexes)\n","        pooled_output = torch.cat((pooled_output, user_emb, topic_emb), dim=1)\n","        pooled_output = nn.ReLU()(pooled_output)  \n","        pooled_output = self.dropout(pooled_output) \n","        logits = self.fc2(pooled_output)\n","        return logits\n","\n","\n","        "]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:56:23.725398Z","iopub.status.busy":"2022-04-29T00:56:23.725123Z","iopub.status.idle":"2022-04-29T00:56:26.086554Z","shell.execute_reply":"2022-04-29T00:56:26.085644Z","shell.execute_reply.started":"2022-04-29T00:56:23.725362Z"},"trusted":true},"outputs":[],"source":["model2 = AutoModel.from_pretrained(modelname)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:56:26.090233Z","iopub.status.busy":"2022-04-29T00:56:26.089844Z","iopub.status.idle":"2022-04-29T00:56:26.094674Z","shell.execute_reply":"2022-04-29T00:56:26.093797Z","shell.execute_reply.started":"2022-04-29T00:56:26.090198Z"},"trusted":true},"outputs":[],"source":["batch_size = 50"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:56:26.097059Z","iopub.status.busy":"2022-04-29T00:56:26.096529Z","iopub.status.idle":"2022-04-29T00:56:34.203699Z","shell.execute_reply":"2022-04-29T00:56:34.199925Z","shell.execute_reply.started":"2022-04-29T00:56:26.097018Z"},"trusted":true},"outputs":[],"source":["# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","# train_dataset = train_dataset[:int(len(list(train_dataset))/50)]\n","# model2 = AutoModel.from_pretrained(modelname)\n","train_dataloader = DataLoader(\n","#             train_dataset,  # The training samples.\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","\n","# val_dataset =    val_dataset[:int(len(list(val_dataset))/50)]\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","device = 'cpu'\n","if torch.cuda.is_available():\n","    device = 'cuda'+':'+str(deviceno)\n","model2 = model2.to(device)\n","print(device)\n","# model = BERTCascade(model2, nn.Parameter(torch.tensor(emb['G'].astype(np.float32)).to(device)))\n","torchuseremb = nn.Parameter(torch.tensor(user_embeddings.astype(np.float32)).to(device))\n","torchtopicemb = nn.Parameter(torch.tensor(topic_embeddings.astype(np.float32)).to(device))\n","model = BERTCascade(model2, torchuseremb, torchtopicemb)\n","\n","model = model.to(device)\n","# Tell pytorch to run this model on the GPU.\n","# model.cuda()\n","\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","print(device)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T00:56:34.205909Z","iopub.status.busy":"2022-04-29T00:56:34.205608Z","iopub.status.idle":"2022-04-29T01:28:28.464733Z","shell.execute_reply":"2022-04-29T01:28:28.464022Z","shell.execute_reply.started":"2022-04-29T00:56:34.205871Z"},"trusted":true},"outputs":[],"source":["\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","epochs = 3\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","epochs = 1\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","l = []\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 1000 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        b_userindexes = batch[3].to(device)\n","        b_topicindexes = batch[4].to(device)\n","        # print(b_labels.shape)\n","        # print(b_input_ids.shape) \n","        optimizer.zero_grad()\n","\n","        vals = model(b_input_ids, \n","                            #  token_type_ids=None, \n","                             b_input_mask, \n","                            b_userindexes,\n","                            b_topicindexes)\n","        loss = F.cross_entropy(vals, b_labels)\n","\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        \n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        b_userindexes = batch[3].to(device)\n","        b_topicindexes = batch[4].to(device)\n","\n","\n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","            \n","            vals = model(b_input_ids, \n","                            #  token_type_ids=None, \n","                             b_input_mask, \n","        #                              labels=b_labels,\n","                            b_userindexes,\n","                            b_topicindexes)\n","        #         loss = vals.loss\n","\n","\n","\n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = vals.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        pred_flat = np.argmax(logits, axis=1).flatten()\n","        labels_flat = label_ids.flatten()\n","        l.append(pred_flat == labels_flat)\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","        # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","\n","\n","# Display the table.\n","df_stats"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T01:28:28.466690Z","iopub.status.busy":"2022-04-29T01:28:28.466427Z","iopub.status.idle":"2022-04-29T01:30:33.570352Z","shell.execute_reply":"2022-04-29T01:30:33.569495Z","shell.execute_reply.started":"2022-04-29T01:28:28.466654Z"},"trusted":true},"outputs":[],"source":["t0 = time.time()\n","l = []\n","# Put the model in evaluation mode--the dropout layers behave differently\n","# during evaluation.\n","model.eval()\n","\n","# Tracking variables \n","total_eval_accuracy = 0\n","total_eval_loss = 0\n","nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","for batch in validation_dataloader:\n","\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    b_userindexes = batch[3].to(device)\n","    b_topicindexes = batch[4].to(device)\n","\n","\n","    # Tell pytorch not to bother with constructing the compute graph during\n","    # the forward pass, since this is only needed for backprop (training).\n","    with torch.no_grad():        \n","\n","        vals = model(b_input_ids, \n","                        #  token_type_ids=None, \n","                         b_input_mask, \n","    #                              labels=b_labels,\n","                        b_userindexes,\n","                        b_topicindexes)\n","    #         loss = vals.loss\n","\n","\n","\n","    # Accumulate the validation loss.\n","    total_eval_loss += loss.item()\n","\n","    # Move logits and labels to CPU\n","    logits = vals.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Calculate the accuracy for this batch of test sentences, and\n","    # accumulate it over all batches.\n","    pred_flat = np.argmax(logits, axis=1).flatten()\n","    labels_flat = label_ids.flatten()\n","    l.append(pred_flat == labels_flat)\n","    total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","    # Report the final accuracy for this validation run.\n","avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","# Calculate the average loss over all of the batches.\n","avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","# Measure how long the validation run took.\n","validation_time = format_time(time.time() - t0)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T01:47:15.524661Z","iopub.status.busy":"2022-04-29T01:47:15.524042Z","iopub.status.idle":"2022-04-29T01:49:20.680232Z","shell.execute_reply":"2022-04-29T01:49:20.679385Z","shell.execute_reply.started":"2022-04-29T01:47:15.524624Z"},"trusted":true},"outputs":[],"source":["t0 = time.time()\n","l = []\n","preds = []\n","label = []\n","# Put the model in evaluation mode--the dropout layers behave differently\n","# during evaluation.\n","model.eval()\n","\n","# Tracking variables \n","total_eval_accuracy = 0\n","total_eval_loss = 0\n","nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","for batch in validation_dataloader:\n","\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    b_userindexes = batch[3].to(device)\n","    b_topicindexes = batch[4].to(device)\n","\n","\n","    # Tell pytorch not to bother with constructing the compute graph during\n","    # the forward pass, since this is only needed for backprop (training).\n","    with torch.no_grad():        \n","\n","        vals = model(b_input_ids, \n","                        #  token_type_ids=None, \n","                         b_input_mask, \n","    #                              labels=b_labels,\n","                        b_userindexes,\n","                        b_topicindexes)\n","    #         loss = vals.loss\n","\n","\n","    # Accumulate the validation loss.\n","    total_eval_loss += loss.item()\n","\n","    # Move logits and labels to CPU\n","    logits = vals.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Calculate the accuracy for this batch of test sentences, and\n","    # accumulate it over all batches.\n","    pred_flat = np.argmax(logits, axis=1).flatten()\n","    labels_flat = label_ids.flatten()\n","    preds.append(pred_flat)\n","    label.append(labels_flat)\n","    l.append(pred_flat == labels_flat)\n","    total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","    # Report the final accuracy for this validation run.\n","avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","# Calculate the average loss over all of the batches.\n","avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","# Measure how long the validation run took.\n","validation_time = format_time(time.time() - t0)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T01:57:46.553767Z","iopub.status.busy":"2022-04-29T01:57:46.553100Z","iopub.status.idle":"2022-04-29T01:57:47.198830Z","shell.execute_reply":"2022-04-29T01:57:47.197762Z","shell.execute_reply.started":"2022-04-29T01:57:46.553728Z"},"trusted":true},"outputs":[],"source":["preds = np.hstack(preds)\n","label = np.hstack(label)\n","from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T02:01:22.036729Z","iopub.status.busy":"2022-04-29T02:01:22.036367Z","iopub.status.idle":"2022-04-29T02:01:22.332468Z","shell.execute_reply":"2022-04-29T02:01:22.331698Z","shell.execute_reply.started":"2022-04-29T02:01:22.036692Z"},"trusted":true},"outputs":[],"source":["cf_matrix = confusion_matrix(label, preds, normalize='all')\n","\n","import seaborn as sns\n","ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n","            fmt='.2%', cmap='Blues')\n","\n","ax.set_title('BERT+CASCADE Response Comment Confusion Matrix\\n\\n')\n","ax.set_xlabel('\\nPredicted Values')\n","ax.set_ylabel('Actual Values ')\n","\n","## Ticket labels - List must be in alphabetical order\n","ax.xaxis.set_ticklabels(['False','True'])\n","ax.yaxis.set_ticklabels(['False','True'])\n","\n","## Display the visualization of the Confusion Matrix.\n","plt.show()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T01:30:33.572312Z","iopub.status.busy":"2022-04-29T01:30:33.572016Z","iopub.status.idle":"2022-04-29T01:30:47.383488Z","shell.execute_reply":"2022-04-29T01:30:47.382709Z","shell.execute_reply.started":"2022-04-29T01:30:33.572274Z"},"trusted":true},"outputs":[],"source":["val2 = np.hstack(l)\n","\n","lr_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2))),('clf',  LogisticRegression(random_state= 42, solver='liblinear'))])\n","lr_clf.fit(train_sentences,train_labels)\n","# print(f\"The accuracy on the training set is: {lr_clf.score(X_train,Y_train)}\")\n","print(f\"The accuracy for our Logistic Regression model is on the test set is:  {lr_clf.score(test_sentences,test_labels)}\")\n","Y_preds = lr_clf.predict(test_sentences)\n","equals = test_labels==Y_preds"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T01:30:47.393236Z","iopub.status.busy":"2022-04-29T01:30:47.392785Z","iopub.status.idle":"2022-04-29T01:30:47.403089Z","shell.execute_reply":"2022-04-29T01:30:47.402428Z","shell.execute_reply.started":"2022-04-29T01:30:47.393200Z"},"trusted":true},"outputs":[],"source":["first = np.sum((np.vstack((equals, val2)).T==np.array([True, True])).all(axis=1))\n","second = np.sum((np.vstack((equals, val2)).T==np.array([True, False])).all(axis=1))\n","third = np.sum((np.vstack((equals, val2)).T==np.array([False, True])).all(axis=1))\n","fourth = np.sum((np.vstack((equals, val2)).T==np.array([False, False])).all(axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["table = [[first, second],[third, fourth]]\n","print('Contingency table', np.array(table)/len(equals))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T01:30:47.404746Z","iopub.status.busy":"2022-04-29T01:30:47.404427Z","iopub.status.idle":"2022-04-29T01:30:59.043945Z","shell.execute_reply":"2022-04-29T01:30:59.042867Z","shell.execute_reply.started":"2022-04-29T01:30:47.404679Z"},"trusted":true},"outputs":[],"source":["# !pip install statsmodels\n","from statsmodels.stats.contingency_tables import mcnemar\n","# define contingency table\n","\n","# calculate mcnemar test\n","result = mcnemar(table, exact=True)\n","# summarize the finding\n","print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n","# interpret the p-value\n","alpha = 0.05\n","if result.pvalue > alpha:\n","    print('Same proportions of errors (fail to reject H0)')\n","else:\n","    print('Different proportions of errors (reject H0)')"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-04-29T01:30:59.047165Z","iopub.status.busy":"2022-04-29T01:30:59.046664Z","iopub.status.idle":"2022-04-29T01:30:59.053604Z","shell.execute_reply":"2022-04-29T01:30:59.052485Z","shell.execute_reply.started":"2022-04-29T01:30:59.047121Z"},"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
