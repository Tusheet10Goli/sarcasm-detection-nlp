{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-05T17:45:59.176833Z","iopub.status.busy":"2022-03-05T17:45:59.175731Z","iopub.status.idle":"2022-03-05T17:45:59.770666Z","shell.execute_reply":"2022-03-05T17:45:59.769944Z","shell.execute_reply.started":"2022-03-05T17:45:59.176794Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","from textblob import TextBlob\n","\n","import seaborn as sns\n","import matplotlib.style as style \n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn import set_config\n","set_config(display=\"diagram\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T17:45:59.772264Z","iopub.status.busy":"2022-03-05T17:45:59.772022Z","iopub.status.idle":"2022-03-05T17:46:08.911113Z","shell.execute_reply":"2022-03-05T17:46:08.910327Z","shell.execute_reply.started":"2022-03-05T17:45:59.772231Z"},"trusted":true},"outputs":[],"source":["datasettype = 'response' # can be either parent, response, or parentresponse"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T17:46:08.914756Z","iopub.status.busy":"2022-03-05T17:46:08.914535Z","iopub.status.idle":"2022-03-05T17:46:15.310570Z","shell.execute_reply":"2022-03-05T17:46:15.309799Z","shell.execute_reply.started":"2022-03-05T17:46:08.914731Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"../data/train-balanced-sarcasm.csv\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T17:46:15.335479Z","iopub.status.busy":"2022-03-05T17:46:15.335251Z","iopub.status.idle":"2022-03-05T17:46:15.578440Z","shell.execute_reply":"2022-03-05T17:46:15.577715Z","shell.execute_reply.started":"2022-03-05T17:46:15.335447Z"},"trusted":true},"outputs":[],"source":["df = df.dropna()\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T17:47:51.329655Z","iopub.status.busy":"2022-03-05T17:47:51.327176Z","iopub.status.idle":"2022-03-05T17:47:58.645159Z","shell.execute_reply":"2022-03-05T17:47:58.644384Z","shell.execute_reply.started":"2022-03-05T17:47:51.329616Z"},"trusted":true},"outputs":[],"source":["\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","import random\n","import numpy as np\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForSequenceClassification, AdamW, BertConfig, AutoModelForSequenceClassification\n","datasettouse = None\n","if datasettype == 'response':\n","    datasettouse = df['comment'].astype(str)\n","elif datasettype == 'parent':\n","    datasettouse = df['parent_comment'].astype(str)\n","elif datasettype == 'parentresponse':\n","    datasettouse = df['parent_comment'].astype(str)+\" \"+df['comment'].astype(str)\n","X_train,X_test,Y_train,Y_test = train_test_split(datasettouse,random_state=42)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T17:47:58.652367Z","iopub.status.busy":"2022-03-05T17:47:58.650051Z","iopub.status.idle":"2022-03-05T17:48:02.454432Z","shell.execute_reply":"2022-03-05T17:48:02.453736Z","shell.execute_reply.started":"2022-03-05T17:47:58.652324Z"},"trusted":true},"outputs":[],"source":["deviceno = 0\n","modelname = 'bert-base-uncased'\n","#modelname = \"cardiffnlp/twitter-roberta-base-offensive\"\n","#modelname = 'microsoft/deberta-base'\n","# modelname = 'facebook/bart-large'\n","#modelname = 'unitary/toxic-bert'\n","max_length = 128\n","batch_size = 8\n","epochs = 4\n","import pandas as pd\n","# df = pd.read_csv('balancedSpaceSep.csv')\n","\n","\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(modelname)\n","\n","import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset\n","# Load the dataset into a pandas dataframe.\n","\n","# Report the number of sentences.\n","# print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Create sentence and label lists\n","train_sentences = X_train\n","test_sentences = X_test\n","train_labels = Y_train\n","test_labels = Y_test\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T18:37:56.257907Z","iopub.status.busy":"2022-03-05T18:37:56.257342Z","iopub.status.idle":"2022-03-05T18:37:56.262445Z","shell.execute_reply":"2022-03-05T18:37:56.261768Z","shell.execute_reply.started":"2022-03-05T18:37:56.257869Z"},"trusted":true},"outputs":[],"source":["\n","len(train_labels)# train_labels = train_labels[:int(len(train_labels)/50)]"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T18:37:58.363759Z","iopub.status.busy":"2022-03-05T18:37:58.363072Z","iopub.status.idle":"2022-03-05T18:38:01.938790Z","shell.execute_reply":"2022-03-05T18:38:01.938038Z","shell.execute_reply.started":"2022-03-05T18:37:58.363715Z"},"trusted":true},"outputs":[],"source":["\n","# For every sentence...\n","input_ids = []\n","attention_masks = []\n","# train_labels = train_labels[:int(len(train_labels)/50)]\n","# train_sentences = train_sentences[:int(len(train_sentences)/50)]\n","for sent in train_sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_length,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(list(train_labels))\n","labels = labels.to(torch.int64)\n","# labels = F.one_hot(labels.to(torch.int64))\n","train_dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","\n"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T18:41:34.016738Z","iopub.status.busy":"2022-03-05T18:41:34.016373Z","iopub.status.idle":"2022-03-05T18:41:34.916103Z","shell.execute_reply":"2022-03-05T18:41:34.915420Z","shell.execute_reply.started":"2022-03-05T18:41:34.016701Z"},"trusted":true},"outputs":[],"source":["\n","input_ids = []\n","attention_masks = []\n","test_labels = test_labels[:int(len(test_labels)/50)]\n","test_sentences = test_sentences[:int(len(test_sentences)/50)]\n","for sent in test_sentences:\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 64,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(list(test_labels))\n","labels = labels.to(torch.int64)\n","# labels = F.one_hot(labels.to(torch.int64))\n","val_dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T18:42:25.108742Z","iopub.status.busy":"2022-03-05T18:42:25.108380Z","iopub.status.idle":"2022-03-05T18:42:27.529951Z","shell.execute_reply":"2022-03-05T18:42:27.529189Z","shell.execute_reply.started":"2022-03-05T18:42:25.108711Z"},"trusted":true},"outputs":[],"source":["\n","train_dataloader = DataLoader(\n","#             train_dataset,  # The training samples.\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = AutoModelForSequenceClassification.from_pretrained(\n","    modelname, # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","# model.cuda()\n","device = 'cpu'\n","if torch.cuda.is_available():\n","    device = 'cuda'+':'+str(deviceno)\n","model = model.to(device)\n","print(device)\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","print(device)\n","\n"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2022-03-05T18:42:35.811974Z","iopub.status.busy":"2022-03-05T18:42:35.811424Z","iopub.status.idle":"2022-03-05T18:59:25.021692Z","shell.execute_reply":"2022-03-05T18:59:25.021025Z","shell.execute_reply.started":"2022-03-05T18:42:35.811939Z"},"trusted":true},"outputs":[],"source":["\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 1\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","\n","        model.zero_grad()        \n","\n","\n","        vals = model(b_input_ids, \n","                            #  token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        loss = vals.loss\n","        logits = vals.logits\n","\n","\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            vals = model(b_input_ids, \n","                            #  token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        loss = vals.loss\n","        logits = vals.logits\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  ACCURACY: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
